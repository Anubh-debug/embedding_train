{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMldlUB+CuxxhlkWTlyyvxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anubh-debug/embedding_train/blob/main/basic_bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "important modules to import. below cell must be executed."
      ],
      "metadata": {
        "id": "G39T_ZPsWLmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset, Dataset\n",
        "from sentence_transformers import losses\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers import SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# use a base model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')"
      ],
      "metadata": {
        "id": "H-jPmvnCWKE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "below code is for softmax loss"
      ],
      "metadata": {
        "id": "xNlxVErKXV88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcnlMinBxG5I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# load mnli dataset from glue\n",
        "train_dataset=load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset=train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "# using softmax loss\n",
        "train_loss = losses.SoftmaxLoss(model=embedding_model, sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimension(), num_labels=3)\n",
        "\n",
        "\n",
        "# create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator=EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]], #making scores between 0 and 1\n",
        "    main_similarity='cosine'\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "args=SentenceTransformerTrainingArguments(\n",
        "    output_dir = \"/content/embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True, #computations will be formed on 16-bit floating point numbers.\n",
        "    eval_steps=100,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "# training our model\n",
        "trainer=SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating our embedding model\n",
        "evaluator(embedding_model)"
      ],
      "metadata": {
        "id": "9AZlLjGNXL1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are going to try two more loss functions: cosine loss and MNR loss"
      ],
      "metadata": {
        "id": "MW7C03e-StjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cosine loss"
      ],
      "metadata": {
        "id": "hMVeEcjoXZoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "# for cosine similarity, we are going to use only two labels.\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\n",
        "    \"glue\", \"mnli\", split=\"train\"\n",
        ").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")\n",
        "mapping={0:1, 1:0, 2:0}\n",
        "train_dataset=Dataset.from_dict(\n",
        "    {\n",
        "        \"sentence1\": train_dataset[\"premise\"],\n",
        "        \"sentence2\": train_dataset[\"hypothesis\"],\n",
        "        \"label\": [float(mapping[label]) for label in train_dataset[\"label\"]]\n",
        "    }\n",
        ")\n",
        "\n",
        "# cosine loss\n",
        "train_loss=losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# again load evaluator\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator=EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity='cosine'\n",
        ")\n",
        "\n",
        "# training_args\n",
        "# Define training arguments\n",
        "args=SentenceTransformerTrainingArguments(\n",
        "    output_dir = \"/content/cosine_loss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True, #computations will be formed on 16-bit floating point numbers.\n",
        "    eval_steps=100,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer=SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MGTkS9XGS3EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator(embedding_model)\n",
        "\n",
        "# output: {'pearson_cosine': 0.7265418343568838, 'spearman_cosine': 0.7280083600575848}"
      ],
      "metadata": {
        "id": "IG_4aby4UwIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using multiple negative ranking loss"
      ],
      "metadata": {
        "id": "WpxEhmpzboOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "mnli = mnli.remove_columns(\"idx\")\n",
        "mnli=mnli.filter(lambda x: True if x[\"label\"]==0 else False) #keeping only entailment\n",
        "\n",
        "# prepare data and soft negative\n",
        "train_dataset={\"anchor\": [], \"positive\": [], \"negative\": []}\n",
        "soft_negatives=list(mnli['hypothesis'])\n",
        "random.shuffle(soft_negatives)\n",
        "\n",
        "for row, soft_negative in tqdm(zip(mnli, soft_negatives)):\n",
        "  train_dataset['anchor'].append(row['premise'])\n",
        "  train_dataset['positive'].append(row['hypothesis'])\n",
        "  train_dataset['negative'].append(soft_negative)\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "\n",
        "# let's define the evaluator\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")\n",
        "\n",
        "# defining train loss\n",
        "train_loss=losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "# defining training args\n",
        "args=SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"mnrloss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "# defining trainer\n",
        "trainer=SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ipnu60xgYwN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator(embedding_model)\n",
        "output: {'pearson_cosine': 0.8091421864243222, 'spearman_cosine': 0.8134963799078724}"
      ],
      "metadata": {
        "id": "fhBKCCH_cN87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}