{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAr2uMWQ1Puj5kUY+EjnLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anubh-debug/embedding_train/blob/main/basic_bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "important modules to import. below cell must be executed."
      ],
      "metadata": {
        "id": "G39T_ZPsWLmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from sentence_transformers import SentenceTransformer, losses, SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sentence_transformers import InputExample\n",
        "from sentence_transformers.datasets import NoDuplicatesDataLoader # Corrected import path\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "H-jPmvnCWKE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use a base model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')"
      ],
      "metadata": {
        "id": "v2jP27l6soqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "below code is for softmax loss"
      ],
      "metadata": {
        "id": "xNlxVErKXV88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcnlMinBxG5I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# load mnli dataset from glue\n",
        "train_dataset=load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset=train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "# using softmax loss\n",
        "train_loss = losses.SoftmaxLoss(model=embedding_model, sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimension(), num_labels=3)\n",
        "\n",
        "\n",
        "# create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator=EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]], #making scores between 0 and 1\n",
        "    main_similarity='cosine'\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "args=SentenceTransformerTrainingArguments(\n",
        "    output_dir = \"/content/embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True, #computations will be formed on 16-bit floating point numbers.\n",
        "    eval_steps=100,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "# training our model\n",
        "trainer=SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating our embedding model\n",
        "evaluator(embedding_model)"
      ],
      "metadata": {
        "id": "9AZlLjGNXL1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are going to try two more loss functions: cosine loss and MNR loss"
      ],
      "metadata": {
        "id": "MW7C03e-StjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cosine loss"
      ],
      "metadata": {
        "id": "hMVeEcjoXZoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "# for cosine similarity, we are going to use only two labels.\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\n",
        "    \"glue\", \"mnli\", split=\"train\"\n",
        ").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")\n",
        "mapping={0:1, 1:0, 2:0}\n",
        "train_dataset=Dataset.from_dict(\n",
        "    {\n",
        "        \"sentence1\": train_dataset[\"premise\"],\n",
        "        \"sentence2\": train_dataset[\"hypothesis\"],\n",
        "        \"label\": [float(mapping[label]) for label in train_dataset[\"label\"]]\n",
        "    }\n",
        ")\n",
        "\n",
        "# cosine loss\n",
        "train_loss=losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# again load evaluator\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator=EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity='cosine'\n",
        ")\n",
        "\n",
        "# training_args\n",
        "# Define training arguments\n",
        "args=SentenceTransformerTrainingArguments(\n",
        "    output_dir = \"/content/cosine_loss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True, #computations will be formed on 16-bit floating point numbers.\n",
        "    eval_steps=100,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer=SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MGTkS9XGS3EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator(embedding_model)\n",
        "\n",
        "# output: {'pearson_cosine': 0.7265418343568838, 'spearman_cosine': 0.7280083600575848}"
      ],
      "metadata": {
        "id": "IG_4aby4UwIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using multiple negative ranking loss"
      ],
      "metadata": {
        "id": "WpxEhmpzboOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "mnli = mnli.remove_columns(\"idx\")\n",
        "mnli=mnli.filter(lambda x: True if x[\"label\"]==0 else False) #keeping only entailment\n",
        "\n",
        "# prepare data and soft negative\n",
        "train_dataset={\"anchor\": [], \"positive\": [], \"negative\": []}\n",
        "soft_negatives=list(mnli['hypothesis'])\n",
        "random.shuffle(soft_negatives)\n",
        "\n",
        "for row, soft_negative in tqdm(zip(mnli, soft_negatives)):\n",
        "  train_dataset['anchor'].append(row['premise'])\n",
        "  train_dataset['positive'].append(row['hypothesis'])\n",
        "  train_dataset['negative'].append(soft_negative)\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "\n",
        "# let's define the evaluator\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")\n",
        "\n",
        "# defining train loss\n",
        "train_loss=losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "# defining training args\n",
        "args=SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"mnrloss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "# defining trainer\n",
        "trainer=SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ipnu60xgYwN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator(embedding_model)\n",
        "# output: {'pearson_cosine': 0.8091421864243222, 'spearman_cosine': 0.8134963799078724}"
      ],
      "metadata": {
        "id": "fhBKCCH_cN87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most straightforward way to fine-tune an embedding model is to repeat the process of training our model as we did before but replace the 'bert-base-uncased' with a pretrained sentence-transformers model"
      ],
      "metadata": {
        "id": "eeN9N66Ho3v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset=load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset=train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1 = val_sts[\"sentence1\"],\n",
        "    sentences2 = val_sts[\"sentence2\"],\n",
        "    scores = [score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ],
      "metadata": {
        "id": "VD_sK0Bno4ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir = \"finetuned_embed_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-le0pQvmqdq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator(embedding_model)\n",
        "# output={'pearson_cosine': 0.8492843146977941, 'spearman_cosine': 0.8491189934593896}"
      ],
      "metadata": {
        "id": "9CsvgJNisNUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmented SBert. We are going to train a cross encoder bert with small gold label dataset. Then we will use that cross encoder to label our unlabeled data creating silver label dataset. Then we will fine-tune our BiEncoder bert with gold+silver dataset."
      ],
      "metadata": {
        "id": "kOHs6AzguRYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare a small dataset of 10000 to train our cross encoder\n",
        "dataset=load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000))\n",
        "mapping={2:0, 1:0, 0:1}\n",
        "\n",
        "gold_examples=[\n",
        "    InpputExample(texts=[row['premise'], row['hypothesis']], label=mapping[row[\"label\"]] for row in dataset)\n",
        "]\n",
        "gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size=32)\n",
        "\n",
        "# pandas dataframe for easier data handling\n",
        "gold=pd.DataFrame(\n",
        "    {\n",
        "        \"sentence1\": dataset[\"premise\"],\n",
        "        \"sentence2\": dataset[\"hypothesis\"],\n",
        "        \"label\": [mapping[label] for label in dataset[\"label\"]]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "lUVJy2yMuo9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using this gold labeled dataset, we can train our cross encoder bert"
      ],
      "metadata": {
        "id": "NaMC-Al4yCli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_encoder = CrossEncoder(\"bert-base-uncased\", num_labels=2)\n",
        "cross_encoder.fit(\n",
        "    train_dataloader = gold_dataloader,\n",
        "    epochs=1,\n",
        "    show_progress_bar=True,\n",
        "    warmup_steps=100,\n",
        "    use_amp=False\n",
        ")"
      ],
      "metadata": {
        "id": "-Frm1rAzyKaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training our cross encoder we can use this model to label our silver dataset"
      ],
      "metadata": {
        "id": "ttWsuXlBzhpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "silver_dataset=load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000, 50_000))\n",
        "pairs=list(zip(silver_dataset[\"premise\"], silver_dataset[\"hypothesis\"]))"
      ],
      "metadata": {
        "id": "ULAg2xAHzoB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "label these sentence pairs"
      ],
      "metadata": {
        "id": "UUJjxh450paE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output=cross_encoder.predict(\n",
        "    pairs, apply_softmax=True\n",
        ")\n",
        "\n",
        "silver=pd.DataFrame(\n",
        "    {\n",
        "        \"sentence1\": silver_dataset[\"premise\"],\n",
        "        \"sentence2\": silver_dataset[\"hypothesis\"],\n",
        "        \"label\":np.argmax(output, axis=1)\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "QELOnYev0r2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine gold + silver\n",
        "data = pd.concat([gold, silver], ignore_index=True, axis=0)\n",
        "data = data.drop_duplicates(subset=[\"sentence1\", \"sentence2\"], keep=\"first\")\n",
        "train_dataset = Dataset.from_pandas(data, preserve_index=False)\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ],
      "metadata": {
        "id": "XT6pbbDQ1gsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "embedding_model = SentenceTransformer(\"bert-base-uncased\")\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"augmented_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "G0zK1BqN1sOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator(embedding_model)"
      ],
      "metadata": {
        "id": "u9loJUSX1tz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised Learning: TSDAE"
      ],
      "metadata": {
        "id": "xGZOlVbm3oFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The underlying idea of TSDAE is that we add noise to the input sentence by removing a certain percentage of words from it. This “damaged” sentence is put through an encoder, with a pooling layer on top of it, to map it to a sentence embedding. From this sentence embedding, a decoder tries to reconstruct the original sentence from the “damaged” sentence but without the artificial noise. The main concept here is that the more accurate the sentence embedding is, the more accurate the reconstructed sentence will be."
      ],
      "metadata": {
        "id": "VrHiJ1qP3kru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download additional tokenizer\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "6Q1OZWHZ2Vfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.datasets import DenoisingAutoEncoderDataset\n",
        "\n",
        "# create a flat list of sentences\n",
        "mnli=load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(25_000))\n",
        "flat_sentences = mnli[\"premise\"] + mnli[\"hypothesis\"]\n",
        "\n",
        "# add noise to our input data\n",
        "damaged_data = DenoisingAutoEncoderDataset(list(set(flat_sentences)))\n",
        "\n",
        "# create dataset\n",
        "train_dataset={\"damaged_sentence\"=[], \"original_sentence\"=[]}\n",
        "\n",
        "for data in tqdm(damaged_data):\n",
        "  train_dataset[\"damaged_data\"].append(data.texts[0])\n",
        "  train_dataset[\"original_sentence\"].append(data.texts[1])\n",
        "train_dataset=Dataset.from_dict(train_dataset)"
      ],
      "metadata": {
        "id": "0uBIRdh24AUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ],
      "metadata": {
        "id": "vQavkS6D60SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ei-u5zj265S3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}